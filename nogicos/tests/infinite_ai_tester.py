# -*- coding: utf-8 -*-
"""
NogicOS Infinite AI Tester
æ— é™å¾ªç¯æµ‹è¯•ï¼Œç›´åˆ° AI è®¤ä¸ºäº§å“ç¨³å®š

æ ¸å¿ƒæµç¨‹:
1. AI ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
2. æ‰§è¡Œæµ‹è¯•
3. AI åˆ†æç»“æœ
4. å¦‚æœæœ‰é—®é¢˜ï¼Œè®°å½•å¹¶ç»§ç»­
5. è¿ç»­ N è½®æ— é—®é¢˜ = äº§å“ç¨³å®š

Usage:
    python -m tests.infinite_ai_tester
    python -m tests.infinite_ai_tester --max-rounds 100
    python -m tests.infinite_ai_tester --stability-threshold 5
"""

import sys
import os
import asyncio
import argparse
import json
import time
import traceback
from datetime import datetime
from pathlib import Path
from typing import Optional, List, Dict, Any
from dataclasses import dataclass, field, asdict
from enum import Enum

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))


class TestStatus(Enum):
    PASS = "pass"
    FAIL = "fail"
    ERROR = "error"
    TIMEOUT = "timeout"
    CRASH = "crash"


class Severity(Enum):
    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"


@dataclass
class Issue:
    """å‘ç°çš„é—®é¢˜"""
    type: str
    severity: Severity
    description: str
    test_prompt: str = ""
    traceback: str = ""
    suggestion: str = ""


@dataclass
class TestResult:
    """å•æ¬¡æµ‹è¯•ç»“æœ"""
    id: int
    round: int
    timestamp: str
    prompt: str
    category: str
    status: TestStatus
    response: str = ""
    error: str = ""
    issues: List[Issue] = field(default_factory=list)
    execution_time: float = 0.0


@dataclass
class RoundSummary:
    """å•è½®æµ‹è¯•æ€»ç»“"""
    round: int
    total_tests: int
    passed: int
    failed: int
    issues_found: List[Issue]
    is_stable: bool
    ai_analysis: str = ""


class InfiniteAITester:
    """AI é©±åŠ¨çš„æ— é™æµ‹è¯•å¾ªç¯"""
    
    def __init__(
        self,
        output_dir: str = "tests/infinite_test_results",
        stability_threshold: int = 3,  # è¿ç»­ N è½®æ— é—®é¢˜ = ç¨³å®š
        tests_per_round: int = 10,
        timeout_per_test: int = 60,
    ):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.stability_threshold = stability_threshold
        self.tests_per_round = tests_per_round
        self.timeout_per_test = timeout_per_test
        
        self.agent = None
        self.llm_client = None
        self.model_name = "claude-sonnet-4-20250514"
        
        # Stats
        self.total_rounds = 0
        self.total_tests = 0
        self.total_issues = 0
        self.consecutive_stable_rounds = 0
        self.all_issues: List[Issue] = []
        self.round_summaries: List[RoundSummary] = []
        
        # Session tracking
        self.session_id = f"infinite_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.start_time = time.time()
        
    def init_llm(self):
        """åˆå§‹åŒ– LLM å®¢æˆ·ç«¯"""
        if self.llm_client is not None:
            return self.llm_client
            
        try:
            import anthropic
            
            # Load API key
            api_key = os.environ.get("ANTHROPIC_API_KEY")
            if not api_key:
                try:
                    import api_keys
                    api_keys.setup_env()
                    api_key = os.environ.get("ANTHROPIC_API_KEY")
                except:
                    pass
            
            if not api_key:
                raise ValueError("ANTHROPIC_API_KEY not found")
            
            self.llm_client = anthropic.Anthropic(api_key=api_key)
            print(f"[âœ“] LLM client initialized ({self.model_name})")
            return self.llm_client
            
        except Exception as e:
            print(f"[âœ—] Failed to init LLM: {e}")
            raise
    
    async def init_agent(self):
        """åˆå§‹åŒ–æµ‹è¯• Agent"""
        if self.agent is not None:
            return self.agent
            
        try:
            from engine.agent.react_agent import ReActAgent
            self.agent = ReActAgent()
            print("[âœ“] Test agent initialized")
            return self.agent
        except Exception as e:
            print(f"[âœ—] Failed to init agent: {e}")
            raise
    
    def call_llm(self, prompt: str, system: str = "") -> str:
        """è°ƒç”¨ LLM"""
        client = self.init_llm()
        
        messages = [{"role": "user", "content": prompt}]
        
        response = client.messages.create(
            model=self.model_name,
            max_tokens=4000,
            system=system if system else "ä½ æ˜¯ NogicOS çš„ QA æµ‹è¯•ä¸“å®¶ã€‚",
            messages=messages,
        )
        
        return response.content[0].text
    
    def generate_test_cases(self, round_num: int, previous_issues: List[Issue] = None) -> List[Dict]:
        """ä½¿ç”¨ AI ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹"""
        
        # æ„å»º prompt
        context = f"""
## NogicOS ç®€ä»‹
NogicOS æ˜¯ä¸€ä¸ª AI å·¥ä½œåŠ©æ‰‹ï¼Œæ ¸å¿ƒèƒ½åŠ›ï¼š
- æµè§ˆç½‘é¡µã€æå–æ•°æ®
- æ–‡ä»¶æ“ä½œï¼ˆè¯»ã€å†™ã€æœç´¢ã€æ•´ç†ï¼‰
- æ‰§è¡Œ Shell å‘½ä»¤
- æ™ºèƒ½ä»»åŠ¡è§„åˆ’

## å¯ç”¨å·¥å…·
- navigate: å¯¼èˆªåˆ° URL
- click: ç‚¹å‡»å…ƒç´ 
- type: è¾“å…¥æ–‡æœ¬
- screenshot: æˆªå›¾
- read_file: è¯»å–æ–‡ä»¶
- write_file: å†™å…¥æ–‡ä»¶
- list_directory: åˆ—å‡ºç›®å½•
- shell_execute: æ‰§è¡Œå‘½ä»¤
- grep_search: æœç´¢æ–‡ä»¶å†…å®¹
- glob_search: æŒ‰æ¨¡å¼æœç´¢æ–‡ä»¶

## å½“å‰æµ‹è¯•è½®æ¬¡: {round_num}
"""
        
        if previous_issues:
            context += "\n## ä¸Šä¸€è½®å‘ç°çš„é—®é¢˜:\n"
            for issue in previous_issues[:5]:
                context += f"- [{issue.severity.value}] {issue.type}: {issue.description}\n"
            context += "\nè¯·ç”Ÿæˆèƒ½å¤ŸéªŒè¯è¿™äº›é—®é¢˜æ˜¯å¦å·²ä¿®å¤çš„æµ‹è¯•ç”¨ä¾‹ã€‚\n"
        
        prompt = f"""{context}

è¯·ç”Ÿæˆ {self.tests_per_round} ä¸ªæµ‹è¯•ç”¨ä¾‹ï¼Œè¦†ç›–ä»¥ä¸‹ç±»åˆ«ï¼š
1. æ–‡ä»¶æ“ä½œ (file) - è¯»å†™ã€æœç´¢ã€æ•´ç†
2. Shell å‘½ä»¤ (shell) - æ‰§è¡Œå‘½ä»¤ã€æŸ¥çœ‹ç»“æœ
3. ä¸­æ–‡å¤„ç† (chinese) - è‡ªç„¶è¯­è¨€ç†è§£
4. é”™è¯¯å¤„ç† (error) - è¾¹ç¼˜æƒ…å†µã€å¼‚å¸¸è¾“å…¥
5. å¤æ‚ä»»åŠ¡ (complex) - å¤šæ­¥éª¤ä»»åŠ¡

ä»¥ JSON æ ¼å¼è¿”å›ï¼Œæ¯ä¸ªæµ‹è¯•åŒ…å«:
- prompt: ç”¨æˆ·è¾“å…¥
- category: æµ‹è¯•ç±»åˆ«
- expected_behavior: æœŸæœ›è¡Œä¸º
- risk_level: é£é™©ç­‰çº§ (low/medium/high)

è¿”å›æ ¼å¼:
```json
[
  {{"prompt": "...", "category": "file", "expected_behavior": "...", "risk_level": "low"}},
  ...
]
```
"""
        
        try:
            response = self.call_llm(prompt)
            
            # æå– JSON
            if "```json" in response:
                start = response.find("```json") + 7
                end = response.find("```", start)
                json_str = response[start:end].strip()
            elif "[" in response:
                start = response.find("[")
                end = response.rfind("]") + 1
                json_str = response[start:end]
            else:
                json_str = response
            
            test_cases = json.loads(json_str)
            return test_cases
            
        except Exception as e:
            print(f"[!] Failed to generate test cases: {e}")
            # è¿”å›é»˜è®¤æµ‹è¯•ç”¨ä¾‹
            return self._get_default_test_cases()
    
    def _get_default_test_cases(self) -> List[Dict]:
        """é»˜è®¤æµ‹è¯•ç”¨ä¾‹ï¼ˆAI ç”Ÿæˆå¤±è´¥æ—¶çš„åå¤‡ï¼‰"""
        return [
            {"prompt": "è¯»å– requirements.txt", "category": "file", "expected_behavior": "è¯»å–æ–‡ä»¶å†…å®¹", "risk_level": "low"},
            {"prompt": "å½“å‰ç›®å½•æœ‰ä»€ä¹ˆæ–‡ä»¶", "category": "file", "expected_behavior": "åˆ—å‡ºç›®å½•", "risk_level": "low"},
            {"prompt": "è¿è¡Œ python --version", "category": "shell", "expected_behavior": "æ˜¾ç¤º Python ç‰ˆæœ¬", "risk_level": "low"},
            {"prompt": "æœç´¢åŒ…å« import çš„ python æ–‡ä»¶", "category": "file", "expected_behavior": "æœç´¢æ–‡ä»¶", "risk_level": "low"},
            {"prompt": "å¸®æˆ‘çœ‹çœ‹é¡¹ç›®ç»“æ„", "category": "chinese", "expected_behavior": "åˆ†æé¡¹ç›®", "risk_level": "medium"},
            {"prompt": "è¯»å–ä¸€ä¸ªä¸å­˜åœ¨çš„æ–‡ä»¶", "category": "error", "expected_behavior": "ä¼˜é›…å¤„ç†é”™è¯¯", "risk_level": "low"},
            {"prompt": "", "category": "error", "expected_behavior": "å¤„ç†ç©ºè¾“å…¥", "risk_level": "low"},
            {"prompt": "ä½ å¥½", "category": "chinese", "expected_behavior": "è‡ªç„¶å›å¤", "risk_level": "low"},
            {"prompt": "åˆ†æ README.md çš„å†…å®¹", "category": "complex", "expected_behavior": "æ€»ç»“æ–‡ä»¶", "risk_level": "medium"},
            {"prompt": "git status", "category": "shell", "expected_behavior": "æ˜¾ç¤º git çŠ¶æ€", "risk_level": "low"},
        ]
    
    async def run_single_test(self, test_case: Dict, test_id: int, round_num: int) -> TestResult:
        """æ‰§è¡Œå•ä¸ªæµ‹è¯•"""
        agent = await self.init_agent()
        
        result = TestResult(
            id=test_id,
            round=round_num,
            timestamp=datetime.now().isoformat(),
            prompt=test_case.get("prompt", ""),
            category=test_case.get("category", "unknown"),
            status=TestStatus.PASS,
        )
        
        start_time = time.time()
        
        try:
            # å¤„ç†ç©ºè¾“å…¥
            task = test_case.get("prompt", "").strip()
            if not task:
                task = "(empty input)"
            
            # æ‰§è¡Œæµ‹è¯•
            agent_result = await asyncio.wait_for(
                agent.run(
                    task=task,
                    session_id=f"{self.session_id}_test_{test_id}",
                ),
                timeout=self.timeout_per_test,
            )
            
            result.response = agent_result.response[:2000] if agent_result.response else ""
            result.execution_time = time.time() - start_time
            
            # åˆ†æå“åº”ä¸­çš„é—®é¢˜
            issues = self._detect_issues(test_case, agent_result)
            result.issues = issues
            
            if issues:
                # åˆ¤æ–­æœ€ä¸¥é‡çš„é—®é¢˜ç­‰çº§
                severities = [i.severity for i in issues]
                if Severity.CRITICAL in severities:
                    result.status = TestStatus.CRASH
                elif Severity.HIGH in severities:
                    result.status = TestStatus.FAIL
                else:
                    result.status = TestStatus.FAIL
            else:
                result.status = TestStatus.PASS
                
        except asyncio.TimeoutError:
            result.status = TestStatus.TIMEOUT
            result.error = f"Timeout after {self.timeout_per_test}s"
            result.execution_time = self.timeout_per_test
            result.issues.append(Issue(
                type="timeout",
                severity=Severity.HIGH,
                description=f"Test timed out after {self.timeout_per_test} seconds",
                test_prompt=test_case.get("prompt", ""),
            ))
            
        except Exception as e:
            result.status = TestStatus.CRASH
            result.error = str(e)
            result.execution_time = time.time() - start_time
            tb = traceback.format_exc()
            result.issues.append(Issue(
                type="crash",
                severity=Severity.CRITICAL,
                description=f"Agent crashed: {str(e)[:200]}",
                test_prompt=test_case.get("prompt", ""),
                traceback=tb[:1000],
            ))
        
        return result
    
    def _detect_issues(self, test_case: Dict, agent_result) -> List[Issue]:
        """æ£€æµ‹å“åº”ä¸­çš„é—®é¢˜"""
        issues = []
        response = agent_result.response or ""
        prompt = test_case.get("prompt", "")
        
        # 1. Python traceback æ³„éœ²
        if "Traceback (most recent call last)" in response:
            issues.append(Issue(
                type="traceback_leaked",
                severity=Severity.HIGH,
                description="Python traceback leaked to user response",
                test_prompt=prompt,
            ))
        
        # 2. ç©ºå“åº”ï¼ˆéç©ºè¾“å…¥æ—¶ï¼‰
        if not response.strip() and prompt.strip():
            issues.append(Issue(
                type="empty_response",
                severity=Severity.MEDIUM,
                description="Empty response for non-empty input",
                test_prompt=prompt,
            ))
        
        # 3. ç¼–ç é—®é¢˜
        encoding_markers = ["\\x", "é”Ÿæ–¤æ‹·", "çƒ«çƒ«çƒ«", "\\u0000"]
        for marker in encoding_markers:
            if marker in response:
                issues.append(Issue(
                    type="encoding_error",
                    severity=Severity.MEDIUM,
                    description=f"Encoding issue detected: {marker}",
                    test_prompt=prompt,
                ))
                break
        
        # 4. æœªå¤„ç†çš„å¼‚å¸¸
        error_patterns = [
            ("KeyError:", Severity.MEDIUM),
            ("AttributeError:", Severity.MEDIUM),
            ("TypeError:", Severity.MEDIUM),
            ("IndexError:", Severity.MEDIUM),
            ("FileNotFoundError:", Severity.LOW),
            ("ConnectionError:", Severity.MEDIUM),
            ("JSONDecodeError:", Severity.MEDIUM),
        ]
        
        for pattern, severity in error_patterns:
            if pattern in response and "sorry" not in response.lower() and "æŠ±æ­‰" not in response:
                issues.append(Issue(
                    type="unhandled_exception",
                    severity=severity,
                    description=f"Unhandled {pattern} in response",
                    test_prompt=prompt,
                ))
        
        # 5. å·¥å…·è°ƒç”¨æ ¼å¼é”™è¯¯
        if ("<tool_call>" in response or "<function_calls>" in response):
            if response.count("<") != response.count(">"):
                issues.append(Issue(
                    type="malformed_tool_call",
                    severity=Severity.HIGH,
                    description="Malformed XML in tool calls",
                    test_prompt=prompt,
                ))
        
        # 6. å®‰å…¨é£é™©
        dangerous_patterns = ["rm -rf", "del /f /q", "format c:", "shutdown", ":(){:|:&};:"]
        for pattern in dangerous_patterns:
            if pattern.lower() in response.lower():
                issues.append(Issue(
                    type="safety_risk",
                    severity=Severity.CRITICAL,
                    description=f"Dangerous pattern detected: {pattern}",
                    test_prompt=prompt,
                ))
        
        return issues
    
    def analyze_round_with_ai(self, round_num: int, results: List[TestResult]) -> RoundSummary:
        """ä½¿ç”¨ AI åˆ†æä¸€è½®æµ‹è¯•ç»“æœ"""
        
        # æ”¶é›†æ•°æ®
        passed = sum(1 for r in results if r.status == TestStatus.PASS)
        failed = len(results) - passed
        all_issues = []
        for r in results:
            all_issues.extend(r.issues)
        
        # æ„å»ºåˆ†æ prompt
        issues_text = ""
        if all_issues:
            issues_text = "\nå‘ç°çš„é—®é¢˜:\n"
            for i, issue in enumerate(all_issues[:10], 1):
                issues_text += f"{i}. [{issue.severity.value}] {issue.type}: {issue.description}\n"
                if issue.test_prompt:
                    issues_text += f"   è§¦å‘è¾“å…¥: {issue.test_prompt[:50]}...\n"
        
        prompt = f"""
åˆ†æç¬¬ {round_num} è½®æµ‹è¯•ç»“æœ:

## ç»Ÿè®¡
- æ€»æµ‹è¯•æ•°: {len(results)}
- é€šè¿‡: {passed}
- å¤±è´¥: {failed}
- é€šè¿‡ç‡: {passed/len(results)*100:.1f}%

{issues_text}

## è¯·å›ç­”
1. è¿™è½®æµ‹è¯•æ˜¯å¦æš´éœ²äº†ä¸¥é‡é—®é¢˜ï¼Ÿ
2. äº§å“æ˜¯å¦å¯ä»¥è®¤ä¸ºç¨³å®šï¼Ÿï¼ˆè¿ç»­ {self.stability_threshold} è½®æ— é«˜å±é—®é¢˜ = ç¨³å®šï¼‰
3. ä¸‹ä¸€è½®åº”è¯¥é‡ç‚¹æµ‹è¯•ä»€ä¹ˆï¼Ÿ

ä»¥ JSON æ ¼å¼è¿”å›:
```json
{{
  "is_stable": true/false,
  "stability_reason": "åˆ¤æ–­ç†ç”±",
  "critical_issues": ["å…³é”®é—®é¢˜1", "..."],
  "next_focus": ["ä¸‹è½®é‡ç‚¹1", "..."],
  "overall_assessment": "æ•´ä½“è¯„ä¼°"
}}
```
"""
        
        try:
            response = self.call_llm(prompt)
            
            # æå– JSON
            if "```json" in response:
                start = response.find("```json") + 7
                end = response.find("```", start)
                json_str = response[start:end].strip()
            else:
                json_str = response
            
            analysis = json.loads(json_str)
            is_stable = analysis.get("is_stable", False) and not any(
                i.severity in [Severity.CRITICAL, Severity.HIGH] for i in all_issues
            )
            
            return RoundSummary(
                round=round_num,
                total_tests=len(results),
                passed=passed,
                failed=failed,
                issues_found=all_issues,
                is_stable=is_stable,
                ai_analysis=analysis.get("overall_assessment", ""),
            )
            
        except Exception as e:
            print(f"[!] AI analysis failed: {e}")
            # åŸºäºè§„åˆ™åˆ¤æ–­
            is_stable = not any(
                i.severity in [Severity.CRITICAL, Severity.HIGH] for i in all_issues
            )
            
            return RoundSummary(
                round=round_num,
                total_tests=len(results),
                passed=passed,
                failed=failed,
                issues_found=all_issues,
                is_stable=is_stable,
                ai_analysis=f"Rule-based: {passed}/{len(results)} passed",
            )
    
    async def run_round(self, round_num: int) -> RoundSummary:
        """æ‰§è¡Œä¸€è½®æµ‹è¯•"""
        print(f"\n{'='*60}")
        print(f"ROUND {round_num}")
        print(f"{'='*60}")
        
        # 1. ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
        print("\n[1/3] Generating test cases with AI...")
        previous_issues = self.all_issues[-20:] if self.all_issues else None
        test_cases = self.generate_test_cases(round_num, previous_issues)
        print(f"      Generated {len(test_cases)} test cases")
        
        # 2. æ‰§è¡Œæµ‹è¯•
        print(f"\n[2/3] Running tests...")
        results: List[TestResult] = []
        
        for i, test_case in enumerate(test_cases):
            prompt_preview = test_case.get("prompt", "")[:40]
            print(f"  [{i+1}/{len(test_cases)}] {test_case.get('category', '?')}: {prompt_preview}...")
            
            result = await self.run_single_test(test_case, i, round_num)
            results.append(result)
            
            # æ˜¾ç¤ºç»“æœ
            status_icon = {
                TestStatus.PASS: "âœ“",
                TestStatus.FAIL: "âœ—",
                TestStatus.ERROR: "!",
                TestStatus.TIMEOUT: "â±",
                TestStatus.CRASH: "ğŸ’¥",
            }.get(result.status, "?")
            
            try:
                print(f"       {status_icon} {result.status.value.upper()} ({result.execution_time:.1f}s)")
            except UnicodeEncodeError:
                print(f"       [{result.status.value.upper()}] ({result.execution_time:.1f}s)")
            
            if result.issues:
                for issue in result.issues[:2]:
                    print(f"         - [{issue.severity.value}] {issue.type}")
        
        # 3. AI åˆ†æ
        print(f"\n[3/3] AI analyzing results...")
        summary = self.analyze_round_with_ai(round_num, results)
        
        # æ›´æ–°ç»Ÿè®¡
        self.total_rounds += 1
        self.total_tests += len(results)
        self.all_issues.extend(summary.issues_found)
        self.total_issues += len(summary.issues_found)
        self.round_summaries.append(summary)
        
        # æ›´æ–°è¿ç»­ç¨³å®šè½®æ•°
        if summary.is_stable:
            self.consecutive_stable_rounds += 1
        else:
            self.consecutive_stable_rounds = 0
        
        # æ˜¾ç¤ºè½®æ¬¡æ€»ç»“
        print(f"\n--- Round {round_num} Summary ---")
        print(f"Passed: {summary.passed}/{summary.total_tests}")
        print(f"Issues: {len(summary.issues_found)}")
        print(f"Stable: {'Yes' if summary.is_stable else 'No'}")
        print(f"Consecutive stable rounds: {self.consecutive_stable_rounds}/{self.stability_threshold}")
        if summary.ai_analysis:
            print(f"AI Assessment: {summary.ai_analysis[:100]}...")
        
        # ä¿å­˜ç»“æœ
        self._save_round_results(round_num, results, summary)
        
        return summary
    
    def _save_round_results(self, round_num: int, results: List[TestResult], summary: RoundSummary):
        """ä¿å­˜å•è½®ç»“æœ"""
        output_file = self.output_dir / f"round_{round_num:04d}.json"
        
        data = {
            "round": round_num,
            "timestamp": datetime.now().isoformat(),
            "summary": {
                "total": summary.total_tests,
                "passed": summary.passed,
                "failed": summary.failed,
                "is_stable": summary.is_stable,
                "ai_analysis": summary.ai_analysis,
            },
            "results": [
                {
                    "id": r.id,
                    "prompt": r.prompt,
                    "category": r.category,
                    "status": r.status.value,
                    "execution_time": r.execution_time,
                    "error": r.error,
                    "issues": [
                        {
                            "type": i.type,
                            "severity": i.severity.value,
                            "description": i.description,
                        }
                        for i in r.issues
                    ],
                }
                for r in results
            ],
        }
        
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    def generate_final_report(self) -> str:
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        elapsed = time.time() - self.start_time
        
        report = f"""
{'='*60}
NogicOS INFINITE AI TESTER - FINAL REPORT
{'='*60}

Session: {self.session_id}
Duration: {elapsed/60:.1f} minutes

## Summary
- Total Rounds: {self.total_rounds}
- Total Tests: {self.total_tests}
- Total Issues: {self.total_issues}
- Consecutive Stable Rounds: {self.consecutive_stable_rounds}
- Final Status: {'âœ“ STABLE' if self.consecutive_stable_rounds >= self.stability_threshold else 'âœ— UNSTABLE'}

## Rounds Overview
"""
        
        for summary in self.round_summaries:
            status = "âœ“" if summary.is_stable else "âœ—"
            report += f"  Round {summary.round}: {summary.passed}/{summary.total_tests} passed, {len(summary.issues_found)} issues {status}\n"
        
        # æŒ‰ç±»å‹ç»Ÿè®¡é—®é¢˜
        if self.all_issues:
            report += "\n## Issues by Type\n"
            issue_types: Dict[str, int] = {}
            for issue in self.all_issues:
                issue_types[issue.type] = issue_types.get(issue.type, 0) + 1
            
            for issue_type, count in sorted(issue_types.items(), key=lambda x: -x[1]):
                report += f"  {issue_type}: {count}\n"
        
        report += f"""
## Conclusion
{'Product is STABLE! âœ“' if self.consecutive_stable_rounds >= self.stability_threshold else 'Product needs more work.'}

Results saved to: {self.output_dir}
{'='*60}
"""
        
        return report
    
    async def run(self, max_rounds: int = 100) -> bool:
        """
        è¿è¡Œæ— é™æµ‹è¯•å¾ªç¯
        
        Returns:
            True if product reached stability, False otherwise
        """
        print("\n" + "="*60)
        print("NogicOS INFINITE AI TESTER")
        print("="*60)
        print(f"Stability threshold: {self.stability_threshold} consecutive stable rounds")
        print(f"Tests per round: {self.tests_per_round}")
        print(f"Max rounds: {max_rounds}")
        print(f"Output: {self.output_dir}")
        print("="*60)
        
        # åˆå§‹åŒ–
        self.init_llm()
        await self.init_agent()
        
        round_num = 0
        
        try:
            while round_num < max_rounds:
                round_num += 1
                
                # æ‰§è¡Œä¸€è½®æµ‹è¯•
                summary = await self.run_round(round_num)
                
                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç¨³å®š
                if self.consecutive_stable_rounds >= self.stability_threshold:
                    print(f"\n{'='*60}")
                    print(f"ğŸ‰ PRODUCT IS STABLE!")
                    print(f"   {self.consecutive_stable_rounds} consecutive stable rounds achieved")
                    print(f"{'='*60}")
                    break
                
                # çŸ­æš‚ä¼‘æ¯ï¼ˆé¿å… API é™æµï¼‰
                await asyncio.sleep(1)
                
        except KeyboardInterrupt:
            print("\n\n[!] Test loop interrupted by user")
        
        except Exception as e:
            print(f"\n\n[!] Test loop failed: {e}")
            traceback.print_exc()
        
        finally:
            # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
            report = self.generate_final_report()
            print(report)
            
            # ä¿å­˜æœ€ç»ˆæŠ¥å‘Š
            report_file = self.output_dir / "final_report.txt"
            with open(report_file, "w", encoding="utf-8") as f:
                f.write(report)
            
            # ä¿å­˜å®Œæ•´æ•°æ®
            full_data_file = self.output_dir / "full_results.json"
            with open(full_data_file, "w", encoding="utf-8") as f:
                json.dump({
                    "session_id": self.session_id,
                    "total_rounds": self.total_rounds,
                    "total_tests": self.total_tests,
                    "total_issues": self.total_issues,
                    "consecutive_stable_rounds": self.consecutive_stable_rounds,
                    "is_stable": self.consecutive_stable_rounds >= self.stability_threshold,
                    "all_issues": [
                        {
                            "type": i.type,
                            "severity": i.severity.value,
                            "description": i.description,
                            "test_prompt": i.test_prompt,
                        }
                        for i in self.all_issues
                    ],
                }, f, ensure_ascii=False, indent=2)
        
        return self.consecutive_stable_rounds >= self.stability_threshold


async def main():
    parser = argparse.ArgumentParser(description="NogicOS Infinite AI Tester")
    parser.add_argument("--max-rounds", type=int, default=100, help="Maximum number of test rounds")
    parser.add_argument("--stability-threshold", type=int, default=3, help="Consecutive stable rounds needed")
    parser.add_argument("--tests-per-round", type=int, default=10, help="Number of tests per round")
    parser.add_argument("--timeout", type=int, default=60, help="Timeout per test in seconds")
    parser.add_argument("--output", type=str, default="tests/infinite_test_results", help="Output directory")
    args = parser.parse_args()
    
    tester = InfiniteAITester(
        output_dir=args.output,
        stability_threshold=args.stability_threshold,
        tests_per_round=args.tests_per_round,
        timeout_per_test=args.timeout,
    )
    
    is_stable = await tester.run(max_rounds=args.max_rounds)
    
    # Exit code: 0 if stable, 1 if not
    sys.exit(0 if is_stable else 1)


if __name__ == "__main__":
    asyncio.run(main())

