# -*- coding: utf-8 -*-
"""
NogicOS Infinite AI Tester with Auto-Fix
æ— é™å¾ªç¯æµ‹è¯• + è‡ªåŠ¨ä¿®å¤ï¼Œç›´åˆ° AI è®¤ä¸ºäº§å“ç¨³å®š

æ ¸å¿ƒæµç¨‹:
1. AI ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
2. æ‰§è¡Œæµ‹è¯•
3. AI åˆ†æç»“æœ
4. å¦‚æœæœ‰é—®é¢˜:
   a. AI å®šä½é—®é¢˜ä»£ç 
   b. AI ç”Ÿæˆä¿®å¤æ–¹æ¡ˆ
   c. å®‰å…¨åº”ç”¨ä¿®å¤ï¼ˆå¸¦å¤‡ä»½ï¼‰
   d. éªŒè¯ä¿®å¤æ˜¯å¦æœ‰æ•ˆ
5. è¿ç»­ N è½®æ— é—®é¢˜ = äº§å“ç¨³å®š

Usage:
    python -m tests.infinite_ai_tester
    python -m tests.infinite_ai_tester --max-rounds 100
    python -m tests.infinite_ai_tester --stability-threshold 5
    python -m tests.infinite_ai_tester --no-fix  # åªæµ‹è¯•ä¸ä¿®å¤
"""

import sys
import os
import asyncio
import argparse
import json
import time
import traceback
from datetime import datetime
from pathlib import Path
from typing import Optional, List, Dict, Any
from dataclasses import dataclass, field, asdict
from enum import Enum

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))


class TestStatus(Enum):
    PASS = "pass"
    FAIL = "fail"
    ERROR = "error"
    TIMEOUT = "timeout"
    CRASH = "crash"


class Severity(Enum):
    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"


@dataclass
class Issue:
    """å‘ç°çš„é—®é¢˜"""
    type: str
    severity: Severity
    description: str
    test_prompt: str = ""
    traceback: str = ""
    suggestion: str = ""
    file_path: str = ""  # é—®é¢˜æ‰€åœ¨æ–‡ä»¶
    line_number: int = 0  # é—®é¢˜æ‰€åœ¨è¡Œå·


@dataclass
class CodeFix:
    """ä»£ç ä¿®å¤æ–¹æ¡ˆ"""
    issue_type: str
    file_path: str
    old_code: str
    new_code: str
    explanation: str
    confidence: float = 0.0  # AI çš„ç½®ä¿¡åº¦ 0-1
    backup_path: str = ""
    applied: bool = False
    verified: bool = False


@dataclass
class TestResult:
    """å•æ¬¡æµ‹è¯•ç»“æœ"""
    id: int
    round: int
    timestamp: str
    prompt: str
    category: str
    status: TestStatus
    response: str = ""
    error: str = ""
    issues: List[Issue] = field(default_factory=list)
    execution_time: float = 0.0


@dataclass
class RoundSummary:
    """å•è½®æµ‹è¯•æ€»ç»“"""
    round: int
    total_tests: int
    passed: int
    failed: int
    issues_found: List[Issue]
    is_stable: bool
    ai_analysis: str = ""


class InfiniteAITester:
    """AI é©±åŠ¨çš„æ— é™æµ‹è¯•å¾ªç¯ï¼ˆå¸¦è‡ªåŠ¨ä¿®å¤ï¼‰"""
    
    def __init__(
        self,
        output_dir: str = "tests/infinite_test_results",
        stability_threshold: int = 3,  # è¿ç»­ N è½®æ— é—®é¢˜ = ç¨³å®š
        tests_per_round: int = 10,
        timeout_per_test: int = 60,
        auto_fix: bool = True,  # æ˜¯å¦è‡ªåŠ¨ä¿®å¤
        max_fix_attempts: int = 3,  # æ¯ä¸ªé—®é¢˜æœ€å¤§ä¿®å¤å°è¯•æ¬¡æ•°
    ):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.stability_threshold = stability_threshold
        self.tests_per_round = tests_per_round
        self.timeout_per_test = timeout_per_test
        self.auto_fix = auto_fix
        self.max_fix_attempts = max_fix_attempts
        
        self.agent = None
        self.llm_client = None
        self.model_name = "claude-sonnet-4-20250514"
        
        # Stats
        self.total_rounds = 0
        self.total_tests = 0
        self.total_issues = 0
        self.consecutive_stable_rounds = 0
        self.all_issues: List[Issue] = []
        self.round_summaries: List[RoundSummary] = []
        
        # ä¿®å¤è¿½è¸ª
        self.fixes_applied: List[CodeFix] = []
        self.fixes_failed: List[CodeFix] = []
        self.fix_attempts: Dict[str, int] = {}  # issue_type -> å°è¯•æ¬¡æ•°
        
        # é¡¹ç›®æ ¹ç›®å½•ï¼ˆç”¨äºå®šä½ä»£ç æ–‡ä»¶ï¼‰
        self.project_root = Path(__file__).parent.parent
        
        # Session tracking
        self.session_id = f"infinite_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.start_time = time.time()
        
    def init_llm(self):
        """åˆå§‹åŒ– LLM å®¢æˆ·ç«¯"""
        if self.llm_client is not None:
            return self.llm_client
            
        try:
            import anthropic
            
            # Load API key
            api_key = os.environ.get("ANTHROPIC_API_KEY")
            if not api_key:
                try:
                    import api_keys
                    api_keys.setup_env()
                    api_key = os.environ.get("ANTHROPIC_API_KEY")
                except:
                    pass
            
            if not api_key:
                raise ValueError("ANTHROPIC_API_KEY not found")
            
            self.llm_client = anthropic.Anthropic(api_key=api_key)
            print(f"[âœ“] LLM client initialized ({self.model_name})")
            return self.llm_client
            
        except Exception as e:
            print(f"[âœ—] Failed to init LLM: {e}")
            raise
    
    async def init_agent(self):
        """åˆå§‹åŒ–æµ‹è¯• Agent"""
        if self.agent is not None:
            return self.agent
            
        try:
            from engine.agent.react_agent import ReActAgent
            self.agent = ReActAgent()
            print("[âœ“] Test agent initialized")
            return self.agent
        except Exception as e:
            print(f"[âœ—] Failed to init agent: {e}")
            raise
    
    def call_llm(self, prompt: str, system: str = "", max_tokens: int = 4000) -> str:
        """è°ƒç”¨ LLM"""
        client = self.init_llm()
        
        response = client.messages.create(
            model=self.model_name,
            max_tokens=max_tokens,
            system=system if system else "ä½ æ˜¯ NogicOS çš„ QA æµ‹è¯•ä¸“å®¶ã€‚",
            messages=[{"role": "user", "content": prompt}],
        )
        
        # æå–æ–‡æœ¬å“åº”
        for block in response.content:
            # æ£€æŸ¥æ˜¯å¦æ˜¯ TextBlock ç±»å‹
            if hasattr(block, "type") and getattr(block, "type", None) == "text":
                return getattr(block, "text", "")
        return ""
    
    def read_file_content(self, file_path: str) -> Optional[str]:
        """å®‰å…¨è¯»å–æ–‡ä»¶å†…å®¹"""
        try:
            full_path = self.project_root / file_path
            if not full_path.exists():
                # å°è¯•ç›´æ¥è·¯å¾„
                full_path = Path(file_path)
            if full_path.exists():
                return full_path.read_text(encoding="utf-8")
        except Exception as e:
            print(f"      [!] Cannot read {file_path}: {e}")
        return None
    
    def locate_issue_in_code(self, issue: Issue, test_result: TestResult) -> Optional[Dict]:
        """ä½¿ç”¨ AI å®šä½é—®é¢˜ä»£ç ä½ç½®"""
        
        # ä» traceback æå–æ–‡ä»¶ä¿¡æ¯
        traceback_text = issue.traceback or test_result.error or ""
        
        # æ„å»ºä»£ç ä¸Šä¸‹æ–‡
        relevant_files = []
        if "engine/" in traceback_text:
            # æå–æ¶‰åŠçš„æ–‡ä»¶
            import re
            file_matches = re.findall(r'File "([^"]+)"', traceback_text)
            for f in file_matches:
                if "engine" in f or "nogicos" in f:
                    relevant_files.append(f)
        
        # é»˜è®¤å¯èƒ½çš„é—®é¢˜æ–‡ä»¶
        if not relevant_files:
            relevant_files = [
                "engine/agent/react_agent.py",
                "engine/tools/browser.py",
                "engine/tools/local.py",
            ]
        
        # è¯»å–ç›¸å…³æ–‡ä»¶å†…å®¹
        file_contents = {}
        for f in relevant_files[:3]:  # æœ€å¤š 3 ä¸ªæ–‡ä»¶
            content = self.read_file_content(f)
            if content:
                # é™åˆ¶é•¿åº¦
                if len(content) > 5000:
                    content = content[:5000] + "\n... (truncated)"
                file_contents[f] = content
        
        if not file_contents:
            return None
        
        prompt = f"""åˆ†æä»¥ä¸‹é”™è¯¯å¹¶ç²¾ç¡®å®šä½é—®é¢˜ä»£ç ä½ç½®ã€‚

## é”™è¯¯ä¿¡æ¯
ç±»å‹: {issue.type}
æè¿°: {issue.description}
è§¦å‘è¾“å…¥: {issue.test_prompt}

## Traceback
```
{traceback_text[:2000]}
```

## ç›¸å…³ä»£ç æ–‡ä»¶
"""
        for filepath, content in file_contents.items():
            prompt += f"\n### {filepath}\n```python\n{content}\n```\n"
        
        prompt += """
## è¯·å®šä½é—®é¢˜
ä»¥ JSON æ ¼å¼è¿”å›:
```json
{
  "file_path": "å…·ä½“æ–‡ä»¶è·¯å¾„",
  "line_start": èµ·å§‹è¡Œå·,
  "line_end": ç»“æŸè¡Œå·,
  "problematic_code": "æœ‰é—®é¢˜çš„ä»£ç ç‰‡æ®µ",
  "root_cause": "é—®é¢˜æ ¹æœ¬åŸå› ",
  "confidence": 0.0-1.0 çš„ç½®ä¿¡åº¦
}
```
"""
        
        try:
            response = self.call_llm(prompt, system="ä½ æ˜¯ä»£ç è°ƒè¯•ä¸“å®¶ï¼Œæ“…é•¿ä»é”™è¯¯æ—¥å¿—å®šä½é—®é¢˜ã€‚")
            
            # æå– JSON
            if "```json" in response:
                start = response.find("```json") + 7
                end = response.find("```", start)
                json_str = response[start:end].strip()
            else:
                json_str = response
            
            return json.loads(json_str)
            
        except Exception as e:
            print(f"      [!] Failed to locate issue: {e}")
            return None
    
    def generate_fix(self, issue: Issue, location: Dict) -> Optional[CodeFix]:
        """ä½¿ç”¨ AI ç”Ÿæˆä¿®å¤ä»£ç """
        
        file_path = location.get("file_path", "")
        if not file_path:
            return None
        
        # è¯»å–å®Œæ•´æ–‡ä»¶å†…å®¹
        file_content = self.read_file_content(file_path)
        if not file_content:
            return None
        
        prompt = f"""æ ¹æ®ä»¥ä¸‹é—®é¢˜åˆ†æï¼Œç”Ÿæˆä¿®å¤ä»£ç ã€‚

## é—®é¢˜
ç±»å‹: {issue.type}
æè¿°: {issue.description}
æ ¹å› : {location.get('root_cause', 'Unknown')}

## é—®é¢˜ä»£ç ä½ç½®
æ–‡ä»¶: {file_path}
è¡Œå·: {location.get('line_start', '?')} - {location.get('line_end', '?')}
é—®é¢˜ä»£ç :
```python
{location.get('problematic_code', '')}
```

## å®Œæ•´æ–‡ä»¶å†…å®¹
```python
{file_content[:8000]}
```

## ä¿®å¤è¦æ±‚
1. åªä¿®æ”¹å¿…è¦çš„éƒ¨åˆ†
2. ä¿æŒä»£ç é£æ ¼ä¸€è‡´
3. æ·»åŠ å¿…è¦çš„é”™è¯¯å¤„ç†
4. ä¸è¦ç ´åç°æœ‰åŠŸèƒ½

## è¿”å›æ ¼å¼
ä»¥ JSON æ ¼å¼è¿”å›ä¿®å¤æ–¹æ¡ˆ:
```json
{{
  "old_code": "éœ€è¦è¢«æ›¿æ¢çš„åŸä»£ç ï¼ˆå®Œæ•´åŒ¹é…ï¼‰",
  "new_code": "æ›¿æ¢åçš„æ–°ä»£ç ",
  "explanation": "ä¿®å¤è¯´æ˜",
  "confidence": 0.0-1.0
}}
```

æ³¨æ„: old_code å¿…é¡»å®Œå…¨åŒ¹é…æ–‡ä»¶ä¸­çš„ä»£ç ï¼ŒåŒ…æ‹¬ç¼©è¿›å’Œç©ºæ ¼ã€‚
"""
        
        try:
            response = self.call_llm(
                prompt, 
                system="ä½ æ˜¯ Python ä¸“å®¶ï¼Œæ“…é•¿ä¿®å¤ä»£ç  bugã€‚ç”Ÿæˆçš„ä»£ç å¿…é¡»å¯ä»¥ç›´æ¥åº”ç”¨ã€‚",
                max_tokens=6000
            )
            
            # æå– JSON
            if "```json" in response:
                start = response.find("```json") + 7
                end = response.find("```", start)
                json_str = response[start:end].strip()
            else:
                json_str = response
            
            fix_data = json.loads(json_str)
            
            return CodeFix(
                issue_type=issue.type,
                file_path=file_path,
                old_code=fix_data.get("old_code", ""),
                new_code=fix_data.get("new_code", ""),
                explanation=fix_data.get("explanation", ""),
                confidence=float(fix_data.get("confidence", 0.5)),
            )
            
        except Exception as e:
            print(f"      [!] Failed to generate fix: {e}")
            return None
    
    def apply_fix_safely(self, fix: CodeFix) -> bool:
        """å®‰å…¨åº”ç”¨ä¿®å¤ï¼ˆå¸¦å¤‡ä»½ï¼‰"""
        
        if not fix.old_code or not fix.new_code:
            print("      [!] Empty fix code")
            return False
        
        try:
            file_path = self.project_root / fix.file_path
            if not file_path.exists():
                file_path = Path(fix.file_path)
            
            if not file_path.exists():
                print(f"      [!] File not found: {fix.file_path}")
                return False
            
            # è¯»å–åŸæ–‡ä»¶
            original_content = file_path.read_text(encoding="utf-8")
            
            # æ£€æŸ¥ old_code æ˜¯å¦å­˜åœ¨
            if fix.old_code not in original_content:
                print(f"      [!] Old code not found in file")
                # å°è¯•æ¨¡ç³ŠåŒ¹é…ï¼ˆå»é™¤ç©ºç™½å·®å¼‚ï¼‰
                normalized_old = " ".join(fix.old_code.split())
                normalized_content = " ".join(original_content.split())
                if normalized_old not in normalized_content:
                    return False
                print(f"      [*] Trying fuzzy match...")
            
            # åˆ›å»ºå¤‡ä»½
            backup_dir = self.output_dir / "backups"
            backup_dir.mkdir(exist_ok=True)
            backup_name = f"{file_path.stem}_{datetime.now().strftime('%H%M%S')}{file_path.suffix}.bak"
            backup_path = backup_dir / backup_name
            backup_path.write_text(original_content, encoding="utf-8")
            fix.backup_path = str(backup_path)
            
            # åº”ç”¨ä¿®å¤
            new_content = original_content.replace(fix.old_code, fix.new_code, 1)
            
            if new_content == original_content:
                print(f"      [!] No changes made")
                return False
            
            # å†™å…¥ä¿®å¤
            file_path.write_text(new_content, encoding="utf-8")
            fix.applied = True
            
            print(f"      [âœ“] Fix applied to {fix.file_path}")
            print(f"          Backup: {backup_path}")
            
            return True
            
        except Exception as e:
            print(f"      [!] Failed to apply fix: {e}")
            return False
    
    def rollback_fix(self, fix: CodeFix) -> bool:
        """å›æ»šä¿®å¤"""
        if not fix.backup_path or not fix.applied:
            return False
        
        try:
            backup_path = Path(fix.backup_path)
            if not backup_path.exists():
                print(f"      [!] Backup not found: {fix.backup_path}")
                return False
            
            file_path = self.project_root / fix.file_path
            if not file_path.exists():
                file_path = Path(fix.file_path)
            
            # æ¢å¤å¤‡ä»½
            backup_content = backup_path.read_text(encoding="utf-8")
            file_path.write_text(backup_content, encoding="utf-8")
            
            fix.applied = False
            print(f"      [â†©] Rolled back {fix.file_path}")
            return True
            
        except Exception as e:
            print(f"      [!] Rollback failed: {e}")
            return False
    
    async def verify_fix(self, fix: CodeFix, original_test: Dict) -> bool:
        """éªŒè¯ä¿®å¤æ˜¯å¦æœ‰æ•ˆ"""
        print(f"      [*] Verifying fix...")
        
        # é‡æ–°è¿è¡ŒåŸæ¥å¤±è´¥çš„æµ‹è¯•
        result = await self.run_single_test(
            original_test, 
            test_id=9999,  # éªŒè¯æµ‹è¯• ID
            round_num=0,
        )
        
        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ç›¸åŒç±»å‹çš„é—®é¢˜
        same_issues = [i for i in result.issues if i.type == fix.issue_type]
        
        if not same_issues and result.status == TestStatus.PASS:
            fix.verified = True
            print(f"      [âœ“] Fix verified - test now passes!")
            return True
        else:
            print(f"      [âœ—] Fix did not resolve the issue")
            return False
    
    async def try_fix_issue(self, issue: Issue, test_result: TestResult, test_case: Dict) -> bool:
        """å°è¯•ä¿®å¤å•ä¸ªé—®é¢˜"""
        
        # æ£€æŸ¥å°è¯•æ¬¡æ•°
        issue_key = f"{issue.type}:{issue.description[:50]}"
        attempts = self.fix_attempts.get(issue_key, 0)
        
        if attempts >= self.max_fix_attempts:
            print(f"      [!] Max fix attempts ({self.max_fix_attempts}) reached for this issue")
            return False
        
        self.fix_attempts[issue_key] = attempts + 1
        
        print(f"      [1/4] Locating issue in code...")
        location = self.locate_issue_in_code(issue, test_result)
        
        if not location:
            print(f"      [!] Could not locate issue")
            return False
        
        print(f"            Found: {location.get('file_path', '?')} line {location.get('line_start', '?')}")
        
        print(f"      [2/4] Generating fix...")
        fix = self.generate_fix(issue, location)
        
        if not fix:
            print(f"      [!] Could not generate fix")
            return False
        
        print(f"            Confidence: {fix.confidence:.0%}")
        
        # ä½ç½®ä¿¡åº¦è­¦å‘Š
        if fix.confidence < 0.5:
            print(f"      [!] Low confidence fix - proceeding with caution")
        
        print(f"      [3/4] Applying fix...")
        if not self.apply_fix_safely(fix):
            self.fixes_failed.append(fix)
            return False
        
        print(f"      [4/4] Verifying fix...")
        # é‡æ–°åˆå§‹åŒ– agent ä»¥åŠ è½½ä¿®å¤åçš„ä»£ç 
        self.agent = None
        
        if await self.verify_fix(fix, test_case):
            self.fixes_applied.append(fix)
            return True
        else:
            # å›æ»š
            print(f"      [!] Fix failed verification - rolling back")
            self.rollback_fix(fix)
            self.fixes_failed.append(fix)
            # é‡æ–°åˆå§‹åŒ– agent
            self.agent = None
            return False
    
    def generate_test_cases(self, round_num: int, previous_issues: Optional[List[Issue]] = None) -> List[Dict]:
        """ä½¿ç”¨ AI ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹"""
        
        # æ„å»º prompt
        context = f"""
## NogicOS ç®€ä»‹
NogicOS æ˜¯ä¸€ä¸ª AI å·¥ä½œåŠ©æ‰‹ï¼Œæ ¸å¿ƒèƒ½åŠ›ï¼š
- æµè§ˆç½‘é¡µã€æå–æ•°æ®
- æ–‡ä»¶æ“ä½œï¼ˆè¯»ã€å†™ã€æœç´¢ã€æ•´ç†ï¼‰
- æ‰§è¡Œ Shell å‘½ä»¤
- æ™ºèƒ½ä»»åŠ¡è§„åˆ’

## å¯ç”¨å·¥å…·
- navigate: å¯¼èˆªåˆ° URL
- click: ç‚¹å‡»å…ƒç´ 
- type: è¾“å…¥æ–‡æœ¬
- screenshot: æˆªå›¾
- read_file: è¯»å–æ–‡ä»¶
- write_file: å†™å…¥æ–‡ä»¶
- list_directory: åˆ—å‡ºç›®å½•
- shell_execute: æ‰§è¡Œå‘½ä»¤
- grep_search: æœç´¢æ–‡ä»¶å†…å®¹
- glob_search: æŒ‰æ¨¡å¼æœç´¢æ–‡ä»¶

## å½“å‰æµ‹è¯•è½®æ¬¡: {round_num}
"""
        
        if previous_issues:
            context += "\n## ä¸Šä¸€è½®å‘ç°çš„é—®é¢˜:\n"
            for issue in previous_issues[:5]:
                context += f"- [{issue.severity.value}] {issue.type}: {issue.description}\n"
            context += "\nè¯·ç”Ÿæˆèƒ½å¤ŸéªŒè¯è¿™äº›é—®é¢˜æ˜¯å¦å·²ä¿®å¤çš„æµ‹è¯•ç”¨ä¾‹ã€‚\n"
        
        prompt = f"""{context}

è¯·ç”Ÿæˆ {self.tests_per_round} ä¸ªæµ‹è¯•ç”¨ä¾‹ï¼Œè¦†ç›–ä»¥ä¸‹ç±»åˆ«ï¼š
1. æ–‡ä»¶æ“ä½œ (file) - è¯»å†™ã€æœç´¢ã€æ•´ç†
2. Shell å‘½ä»¤ (shell) - æ‰§è¡Œå‘½ä»¤ã€æŸ¥çœ‹ç»“æœ
3. ä¸­æ–‡å¤„ç† (chinese) - è‡ªç„¶è¯­è¨€ç†è§£
4. é”™è¯¯å¤„ç† (error) - è¾¹ç¼˜æƒ…å†µã€å¼‚å¸¸è¾“å…¥
5. å¤æ‚ä»»åŠ¡ (complex) - å¤šæ­¥éª¤ä»»åŠ¡

ä»¥ JSON æ ¼å¼è¿”å›ï¼Œæ¯ä¸ªæµ‹è¯•åŒ…å«:
- prompt: ç”¨æˆ·è¾“å…¥
- category: æµ‹è¯•ç±»åˆ«
- expected_behavior: æœŸæœ›è¡Œä¸º
- risk_level: é£é™©ç­‰çº§ (low/medium/high)

è¿”å›æ ¼å¼:
```json
[
  {{"prompt": "...", "category": "file", "expected_behavior": "...", "risk_level": "low"}},
  ...
]
```
"""
        
        try:
            response = self.call_llm(prompt)
            
            # æå– JSON
            if "```json" in response:
                start = response.find("```json") + 7
                end = response.find("```", start)
                json_str = response[start:end].strip()
            elif "[" in response:
                start = response.find("[")
                end = response.rfind("]") + 1
                json_str = response[start:end]
            else:
                json_str = response
            
            test_cases = json.loads(json_str)
            return test_cases
            
        except Exception as e:
            print(f"[!] Failed to generate test cases: {e}")
            # è¿”å›é»˜è®¤æµ‹è¯•ç”¨ä¾‹
            return self._get_default_test_cases()
    
    def _get_default_test_cases(self) -> List[Dict]:
        """é»˜è®¤æµ‹è¯•ç”¨ä¾‹ï¼ˆAI ç”Ÿæˆå¤±è´¥æ—¶çš„åå¤‡ï¼‰"""
        return [
            {"prompt": "è¯»å– requirements.txt", "category": "file", "expected_behavior": "è¯»å–æ–‡ä»¶å†…å®¹", "risk_level": "low"},
            {"prompt": "å½“å‰ç›®å½•æœ‰ä»€ä¹ˆæ–‡ä»¶", "category": "file", "expected_behavior": "åˆ—å‡ºç›®å½•", "risk_level": "low"},
            {"prompt": "è¿è¡Œ python --version", "category": "shell", "expected_behavior": "æ˜¾ç¤º Python ç‰ˆæœ¬", "risk_level": "low"},
            {"prompt": "æœç´¢åŒ…å« import çš„ python æ–‡ä»¶", "category": "file", "expected_behavior": "æœç´¢æ–‡ä»¶", "risk_level": "low"},
            {"prompt": "å¸®æˆ‘çœ‹çœ‹é¡¹ç›®ç»“æ„", "category": "chinese", "expected_behavior": "åˆ†æé¡¹ç›®", "risk_level": "medium"},
            {"prompt": "è¯»å–ä¸€ä¸ªä¸å­˜åœ¨çš„æ–‡ä»¶", "category": "error", "expected_behavior": "ä¼˜é›…å¤„ç†é”™è¯¯", "risk_level": "low"},
            {"prompt": "", "category": "error", "expected_behavior": "å¤„ç†ç©ºè¾“å…¥", "risk_level": "low"},
            {"prompt": "ä½ å¥½", "category": "chinese", "expected_behavior": "è‡ªç„¶å›å¤", "risk_level": "low"},
            {"prompt": "åˆ†æ README.md çš„å†…å®¹", "category": "complex", "expected_behavior": "æ€»ç»“æ–‡ä»¶", "risk_level": "medium"},
            {"prompt": "git status", "category": "shell", "expected_behavior": "æ˜¾ç¤º git çŠ¶æ€", "risk_level": "low"},
        ]
    
    async def run_single_test(self, test_case: Dict, test_id: int, round_num: int) -> TestResult:
        """æ‰§è¡Œå•ä¸ªæµ‹è¯•"""
        agent = await self.init_agent()
        
        result = TestResult(
            id=test_id,
            round=round_num,
            timestamp=datetime.now().isoformat(),
            prompt=test_case.get("prompt", ""),
            category=test_case.get("category", "unknown"),
            status=TestStatus.PASS,
        )
        
        start_time = time.time()
        
        try:
            # å¤„ç†ç©ºè¾“å…¥
            task = test_case.get("prompt", "").strip()
            if not task:
                task = "(empty input)"
            
            # æ‰§è¡Œæµ‹è¯•
            agent_result = await asyncio.wait_for(
                agent.run(
                    task=task,
                    session_id=f"{self.session_id}_test_{test_id}",
                ),
                timeout=self.timeout_per_test,
            )
            
            result.response = agent_result.response[:2000] if agent_result.response else ""
            result.execution_time = time.time() - start_time
            
            # åˆ†æå“åº”ä¸­çš„é—®é¢˜
            issues = self._detect_issues(test_case, agent_result)
            result.issues = issues
            
            if issues:
                # åˆ¤æ–­æœ€ä¸¥é‡çš„é—®é¢˜ç­‰çº§
                severities = [i.severity for i in issues]
                if Severity.CRITICAL in severities:
                    result.status = TestStatus.CRASH
                elif Severity.HIGH in severities:
                    result.status = TestStatus.FAIL
                else:
                    result.status = TestStatus.FAIL
            else:
                result.status = TestStatus.PASS
                
        except asyncio.TimeoutError:
            result.status = TestStatus.TIMEOUT
            result.error = f"Timeout after {self.timeout_per_test}s"
            result.execution_time = self.timeout_per_test
            result.issues.append(Issue(
                type="timeout",
                severity=Severity.HIGH,
                description=f"Test timed out after {self.timeout_per_test} seconds",
                test_prompt=test_case.get("prompt", ""),
            ))
            
        except Exception as e:
            result.status = TestStatus.CRASH
            result.error = str(e)
            result.execution_time = time.time() - start_time
            tb = traceback.format_exc()
            result.issues.append(Issue(
                type="crash",
                severity=Severity.CRITICAL,
                description=f"Agent crashed: {str(e)[:200]}",
                test_prompt=test_case.get("prompt", ""),
                traceback=tb[:1000],
            ))
        
        return result
    
    def _detect_issues(self, test_case: Dict, agent_result) -> List[Issue]:
        """æ£€æµ‹å“åº”ä¸­çš„é—®é¢˜"""
        issues = []
        response = agent_result.response or ""
        prompt = test_case.get("prompt", "")
        
        # 1. Python traceback æ³„éœ²
        if "Traceback (most recent call last)" in response:
            issues.append(Issue(
                type="traceback_leaked",
                severity=Severity.HIGH,
                description="Python traceback leaked to user response",
                test_prompt=prompt,
            ))
        
        # 2. ç©ºå“åº”ï¼ˆéç©ºè¾“å…¥æ—¶ï¼‰
        if not response.strip() and prompt.strip():
            issues.append(Issue(
                type="empty_response",
                severity=Severity.MEDIUM,
                description="Empty response for non-empty input",
                test_prompt=prompt,
            ))
        
        # 3. ç¼–ç é—®é¢˜
        encoding_markers = ["\\x", "é”Ÿæ–¤æ‹·", "çƒ«çƒ«çƒ«", "\\u0000"]
        for marker in encoding_markers:
            if marker in response:
                issues.append(Issue(
                    type="encoding_error",
                    severity=Severity.MEDIUM,
                    description=f"Encoding issue detected: {marker}",
                    test_prompt=prompt,
                ))
                break
        
        # 4. æœªå¤„ç†çš„å¼‚å¸¸
        error_patterns = [
            ("KeyError:", Severity.MEDIUM),
            ("AttributeError:", Severity.MEDIUM),
            ("TypeError:", Severity.MEDIUM),
            ("IndexError:", Severity.MEDIUM),
            ("FileNotFoundError:", Severity.LOW),
            ("ConnectionError:", Severity.MEDIUM),
            ("JSONDecodeError:", Severity.MEDIUM),
        ]
        
        for pattern, severity in error_patterns:
            if pattern in response and "sorry" not in response.lower() and "æŠ±æ­‰" not in response:
                issues.append(Issue(
                    type="unhandled_exception",
                    severity=severity,
                    description=f"Unhandled {pattern} in response",
                    test_prompt=prompt,
                ))
        
        # 5. å·¥å…·è°ƒç”¨æ ¼å¼é”™è¯¯
        if ("<tool_call>" in response or "<function_calls>" in response):
            if response.count("<") != response.count(">"):
                issues.append(Issue(
                    type="malformed_tool_call",
                    severity=Severity.HIGH,
                    description="Malformed XML in tool calls",
                    test_prompt=prompt,
                ))
        
        # 6. å®‰å…¨é£é™©
        dangerous_patterns = ["rm -rf", "del /f /q", "format c:", "shutdown", ":(){:|:&};:"]
        for pattern in dangerous_patterns:
            if pattern.lower() in response.lower():
                issues.append(Issue(
                    type="safety_risk",
                    severity=Severity.CRITICAL,
                    description=f"Dangerous pattern detected: {pattern}",
                    test_prompt=prompt,
                ))
        
        return issues
    
    def analyze_round_with_ai(self, round_num: int, results: List[TestResult]) -> RoundSummary:
        """ä½¿ç”¨ AI åˆ†æä¸€è½®æµ‹è¯•ç»“æœ"""
        
        # æ”¶é›†æ•°æ®
        passed = sum(1 for r in results if r.status == TestStatus.PASS)
        failed = len(results) - passed
        all_issues = []
        for r in results:
            all_issues.extend(r.issues)
        
        # æ„å»ºåˆ†æ prompt
        issues_text = ""
        if all_issues:
            issues_text = "\nå‘ç°çš„é—®é¢˜:\n"
            for i, issue in enumerate(all_issues[:10], 1):
                issues_text += f"{i}. [{issue.severity.value}] {issue.type}: {issue.description}\n"
                if issue.test_prompt:
                    issues_text += f"   è§¦å‘è¾“å…¥: {issue.test_prompt[:50]}...\n"
        
        prompt = f"""
åˆ†æç¬¬ {round_num} è½®æµ‹è¯•ç»“æœ:

## ç»Ÿè®¡
- æ€»æµ‹è¯•æ•°: {len(results)}
- é€šè¿‡: {passed}
- å¤±è´¥: {failed}
- é€šè¿‡ç‡: {passed/len(results)*100:.1f}%

{issues_text}

## è¯·å›ç­”
1. è¿™è½®æµ‹è¯•æ˜¯å¦æš´éœ²äº†ä¸¥é‡é—®é¢˜ï¼Ÿ
2. äº§å“æ˜¯å¦å¯ä»¥è®¤ä¸ºç¨³å®šï¼Ÿï¼ˆè¿ç»­ {self.stability_threshold} è½®æ— é«˜å±é—®é¢˜ = ç¨³å®šï¼‰
3. ä¸‹ä¸€è½®åº”è¯¥é‡ç‚¹æµ‹è¯•ä»€ä¹ˆï¼Ÿ

ä»¥ JSON æ ¼å¼è¿”å›:
```json
{{
  "is_stable": true/false,
  "stability_reason": "åˆ¤æ–­ç†ç”±",
  "critical_issues": ["å…³é”®é—®é¢˜1", "..."],
  "next_focus": ["ä¸‹è½®é‡ç‚¹1", "..."],
  "overall_assessment": "æ•´ä½“è¯„ä¼°"
}}
```
"""
        
        try:
            response = self.call_llm(prompt)
            
            # æå– JSON
            if "```json" in response:
                start = response.find("```json") + 7
                end = response.find("```", start)
                json_str = response[start:end].strip()
            else:
                json_str = response
            
            analysis = json.loads(json_str)
            is_stable = analysis.get("is_stable", False) and not any(
                i.severity in [Severity.CRITICAL, Severity.HIGH] for i in all_issues
            )
            
            return RoundSummary(
                round=round_num,
                total_tests=len(results),
                passed=passed,
                failed=failed,
                issues_found=all_issues,
                is_stable=is_stable,
                ai_analysis=analysis.get("overall_assessment", ""),
            )
            
        except Exception as e:
            print(f"[!] AI analysis failed: {e}")
            # åŸºäºè§„åˆ™åˆ¤æ–­
            is_stable = not any(
                i.severity in [Severity.CRITICAL, Severity.HIGH] for i in all_issues
            )
            
            return RoundSummary(
                round=round_num,
                total_tests=len(results),
                passed=passed,
                failed=failed,
                issues_found=all_issues,
                is_stable=is_stable,
                ai_analysis=f"Rule-based: {passed}/{len(results)} passed",
            )
    
    async def run_round(self, round_num: int) -> RoundSummary:
        """æ‰§è¡Œä¸€è½®æµ‹è¯•"""
        print(f"\n{'='*60}")
        print(f"ROUND {round_num}")
        print(f"{'='*60}")
        
        # 1. ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
        print("\n[1/4] Generating test cases with AI...")
        previous_issues = self.all_issues[-20:] if self.all_issues else None
        test_cases = self.generate_test_cases(round_num, previous_issues)
        print(f"      Generated {len(test_cases)} test cases")
        
        # 2. æ‰§è¡Œæµ‹è¯•
        print(f"\n[2/4] Running tests...")
        results: List[TestResult] = []
        issues_to_fix: List[tuple] = []  # (issue, test_result, test_case)
        
        for i, test_case in enumerate(test_cases):
            prompt_preview = test_case.get("prompt", "")[:40]
            print(f"  [{i+1}/{len(test_cases)}] {test_case.get('category', '?')}: {prompt_preview}...")
            
            result = await self.run_single_test(test_case, i, round_num)
            results.append(result)
            
            # æ˜¾ç¤ºç»“æœ
            status_icon = {
                TestStatus.PASS: "âœ“",
                TestStatus.FAIL: "âœ—",
                TestStatus.ERROR: "!",
                TestStatus.TIMEOUT: "â±",
                TestStatus.CRASH: "ğŸ’¥",
            }.get(result.status, "?")
            
            try:
                print(f"       {status_icon} {result.status.value.upper()} ({result.execution_time:.1f}s)")
            except UnicodeEncodeError:
                print(f"       [{result.status.value.upper()}] ({result.execution_time:.1f}s)")
            
            if result.issues:
                for issue in result.issues[:2]:
                    print(f"         - [{issue.severity.value}] {issue.type}")
                    # æ”¶é›†éœ€è¦ä¿®å¤çš„é«˜å±é—®é¢˜
                    if self.auto_fix and issue.severity in [Severity.CRITICAL, Severity.HIGH]:
                        issues_to_fix.append((issue, result, test_case))
        
        # 3. è‡ªåŠ¨ä¿®å¤ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        fixes_this_round = 0
        if self.auto_fix and issues_to_fix:
            print(f"\n[3/4] Auto-fixing {len(issues_to_fix)} high-severity issues...")
            
            for issue, test_result, test_case in issues_to_fix:
                print(f"\n  Fixing: [{issue.severity.value}] {issue.type}")
                print(f"          {issue.description[:60]}...")
                
                if await self.try_fix_issue(issue, test_result, test_case):
                    fixes_this_round += 1
                    print(f"          âœ“ Fixed successfully!")
                else:
                    print(f"          âœ— Could not fix automatically")
            
            print(f"\n      Fixed {fixes_this_round}/{len(issues_to_fix)} issues this round")
        else:
            print(f"\n[3/4] No auto-fix needed (no high-severity issues or auto-fix disabled)")
        
        # 4. AI åˆ†æ
        print(f"\n[4/4] AI analyzing results...")
        summary = self.analyze_round_with_ai(round_num, results)
        
        # æ›´æ–°ç»Ÿè®¡
        self.total_rounds += 1
        self.total_tests += len(results)
        self.all_issues.extend(summary.issues_found)
        self.total_issues += len(summary.issues_found)
        self.round_summaries.append(summary)
        
        # æ›´æ–°è¿ç»­ç¨³å®šè½®æ•°
        if summary.is_stable:
            self.consecutive_stable_rounds += 1
        else:
            self.consecutive_stable_rounds = 0
        
        # æ˜¾ç¤ºè½®æ¬¡æ€»ç»“
        print(f"\n--- Round {round_num} Summary ---")
        print(f"Passed: {summary.passed}/{summary.total_tests}")
        print(f"Issues: {len(summary.issues_found)}")
        if self.auto_fix:
            print(f"Fixes applied this round: {fixes_this_round}")
            print(f"Total fixes applied: {len(self.fixes_applied)}")
        print(f"Stable: {'Yes' if summary.is_stable else 'No'}")
        print(f"Consecutive stable rounds: {self.consecutive_stable_rounds}/{self.stability_threshold}")
        if summary.ai_analysis:
            print(f"AI Assessment: {summary.ai_analysis[:100]}...")
        
        # ä¿å­˜ç»“æœ
        self._save_round_results(round_num, results, summary)
        
        return summary
    
    def _save_round_results(self, round_num: int, results: List[TestResult], summary: RoundSummary):
        """ä¿å­˜å•è½®ç»“æœ"""
        output_file = self.output_dir / f"round_{round_num:04d}.json"
        
        data = {
            "round": round_num,
            "timestamp": datetime.now().isoformat(),
            "summary": {
                "total": summary.total_tests,
                "passed": summary.passed,
                "failed": summary.failed,
                "is_stable": summary.is_stable,
                "ai_analysis": summary.ai_analysis,
            },
            "results": [
                {
                    "id": r.id,
                    "prompt": r.prompt,
                    "category": r.category,
                    "status": r.status.value,
                    "execution_time": r.execution_time,
                    "error": r.error,
                    "issues": [
                        {
                            "type": i.type,
                            "severity": i.severity.value,
                            "description": i.description,
                        }
                        for i in r.issues
                    ],
                }
                for r in results
            ],
        }
        
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    def generate_final_report(self) -> str:
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        elapsed = time.time() - self.start_time
        
        report = f"""
{'='*60}
NogicOS INFINITE AI TESTER - FINAL REPORT
{'='*60}

Session: {self.session_id}
Duration: {elapsed/60:.1f} minutes
Auto-Fix: {'Enabled' if self.auto_fix else 'Disabled'}

## Summary
- Total Rounds: {self.total_rounds}
- Total Tests: {self.total_tests}
- Total Issues Found: {self.total_issues}
- Fixes Applied: {len(self.fixes_applied)}
- Fixes Failed: {len(self.fixes_failed)}
- Consecutive Stable Rounds: {self.consecutive_stable_rounds}
- Final Status: {'âœ“ STABLE' if self.consecutive_stable_rounds >= self.stability_threshold else 'âœ— UNSTABLE'}

## Rounds Overview
"""
        
        for summary in self.round_summaries:
            status = "âœ“" if summary.is_stable else "âœ—"
            report += f"  Round {summary.round}: {summary.passed}/{summary.total_tests} passed, {len(summary.issues_found)} issues {status}\n"
        
        # æŒ‰ç±»å‹ç»Ÿè®¡é—®é¢˜
        if self.all_issues:
            report += "\n## Issues by Type\n"
            issue_types: Dict[str, int] = {}
            for issue in self.all_issues:
                issue_types[issue.type] = issue_types.get(issue.type, 0) + 1
            
            for issue_type, count in sorted(issue_types.items(), key=lambda x: -x[1]):
                report += f"  {issue_type}: {count}\n"
        
        # ä¿®å¤è¯¦æƒ…
        if self.fixes_applied:
            report += "\n## Fixes Applied\n"
            for i, fix in enumerate(self.fixes_applied, 1):
                report += f"  {i}. [{fix.issue_type}] {fix.file_path}\n"
                report += f"     {fix.explanation[:80]}...\n"
                report += f"     Verified: {'Yes' if fix.verified else 'No'}\n"
        
        if self.fixes_failed:
            report += "\n## Fixes Failed\n"
            for i, fix in enumerate(self.fixes_failed, 1):
                report += f"  {i}. [{fix.issue_type}] {fix.file_path}\n"
                report += f"     {fix.explanation[:80]}...\n" if fix.explanation else ""
        
        report += f"""
## Conclusion
{'Product is STABLE! âœ“' if self.consecutive_stable_rounds >= self.stability_threshold else 'Product needs more work.'}

Results saved to: {self.output_dir}
{'='*60}
"""
        
        return report
    
    async def run(self, max_rounds: int = 100) -> bool:
        """
        è¿è¡Œæ— é™æµ‹è¯•å¾ªç¯
        
        Returns:
            True if product reached stability, False otherwise
        """
        print("\n" + "="*60)
        print("NogicOS INFINITE AI TESTER" + (" + AUTO-FIX" if self.auto_fix else ""))
        print("="*60)
        print(f"Stability threshold: {self.stability_threshold} consecutive stable rounds")
        print(f"Tests per round: {self.tests_per_round}")
        print(f"Max rounds: {max_rounds}")
        print(f"Auto-fix: {'Enabled (max {0} attempts per issue)'.format(self.max_fix_attempts) if self.auto_fix else 'Disabled'}")
        print(f"Output: {self.output_dir}")
        print("="*60)
        
        # åˆå§‹åŒ–
        self.init_llm()
        await self.init_agent()
        
        round_num = 0
        
        try:
            while round_num < max_rounds:
                round_num += 1
                
                # æ‰§è¡Œä¸€è½®æµ‹è¯•
                summary = await self.run_round(round_num)
                
                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç¨³å®š
                if self.consecutive_stable_rounds >= self.stability_threshold:
                    print(f"\n{'='*60}")
                    print(f"ğŸ‰ PRODUCT IS STABLE!")
                    print(f"   {self.consecutive_stable_rounds} consecutive stable rounds achieved")
                    print(f"{'='*60}")
                    break
                
                # çŸ­æš‚ä¼‘æ¯ï¼ˆé¿å… API é™æµï¼‰
                await asyncio.sleep(1)
                
        except KeyboardInterrupt:
            print("\n\n[!] Test loop interrupted by user")
        
        except Exception as e:
            print(f"\n\n[!] Test loop failed: {e}")
            traceback.print_exc()
        
        finally:
            # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
            report = self.generate_final_report()
            print(report)
            
            # ä¿å­˜æœ€ç»ˆæŠ¥å‘Š
            report_file = self.output_dir / "final_report.txt"
            with open(report_file, "w", encoding="utf-8") as f:
                f.write(report)
            
            # ä¿å­˜å®Œæ•´æ•°æ®
            full_data_file = self.output_dir / "full_results.json"
            with open(full_data_file, "w", encoding="utf-8") as f:
                json.dump({
                    "session_id": self.session_id,
                    "total_rounds": self.total_rounds,
                    "total_tests": self.total_tests,
                    "total_issues": self.total_issues,
                    "consecutive_stable_rounds": self.consecutive_stable_rounds,
                    "is_stable": self.consecutive_stable_rounds >= self.stability_threshold,
                    "auto_fix_enabled": self.auto_fix,
                    "fixes_applied": [
                        {
                            "issue_type": f.issue_type,
                            "file_path": f.file_path,
                            "explanation": f.explanation,
                            "verified": f.verified,
                            "backup_path": f.backup_path,
                        }
                        for f in self.fixes_applied
                    ],
                    "fixes_failed": [
                        {
                            "issue_type": f.issue_type,
                            "file_path": f.file_path,
                            "explanation": f.explanation,
                        }
                        for f in self.fixes_failed
                    ],
                    "all_issues": [
                        {
                            "type": i.type,
                            "severity": i.severity.value,
                            "description": i.description,
                            "test_prompt": i.test_prompt,
                        }
                        for i in self.all_issues
                    ],
                }, f, ensure_ascii=False, indent=2)
        
        return self.consecutive_stable_rounds >= self.stability_threshold


async def main():
    parser = argparse.ArgumentParser(description="NogicOS Infinite AI Tester with Auto-Fix")
    parser.add_argument("--max-rounds", type=int, default=100, help="Maximum number of test rounds")
    parser.add_argument("--stability-threshold", type=int, default=3, help="Consecutive stable rounds needed")
    parser.add_argument("--tests-per-round", type=int, default=10, help="Number of tests per round")
    parser.add_argument("--timeout", type=int, default=60, help="Timeout per test in seconds")
    parser.add_argument("--output", type=str, default="tests/infinite_test_results", help="Output directory")
    parser.add_argument("--no-fix", action="store_true", help="Disable auto-fix (test only)")
    parser.add_argument("--max-fix-attempts", type=int, default=3, help="Max fix attempts per issue type")
    args = parser.parse_args()
    
    tester = InfiniteAITester(
        output_dir=args.output,
        stability_threshold=args.stability_threshold,
        tests_per_round=args.tests_per_round,
        timeout_per_test=args.timeout,
        auto_fix=not args.no_fix,
        max_fix_attempts=args.max_fix_attempts,
    )
    
    is_stable = await tester.run(max_rounds=args.max_rounds)
    
    # Exit code: 0 if stable, 1 if not
    sys.exit(0 if is_stable else 1)


if __name__ == "__main__":
    asyncio.run(main())

